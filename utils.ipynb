{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect, DetectorFactory\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from io import BytesIO\n",
    "from PyPDF2 import PdfReader\n",
    "import fitz\n",
    "from sklearn.metrics import classification_report, f1_score, cohen_kappa_score\n",
    "DetectorFactory.seed = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Text From PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_clean_pdf(path, strict=False):\n",
    "    data = open(path, 'rb').read()\n",
    "    start = data.find(b'%PDF-')\n",
    "    end   = data.rfind(b'%%EOF')\n",
    "    if start < 0 or end < 0:\n",
    "        raise ValueError(\"Not a valid PDF (no %PDF- or %%EOF)\")\n",
    "    trimmed = data[start:end + len(b'%%EOF')]\n",
    "    return PdfReader(BytesIO(trimmed), strict=strict)\n",
    "\n",
    "def extract_text(pdf_path, start_page=1, end_page=None):\n",
    "    \"\"\"\n",
    "    Crops the top area of each page and extracts text from the cropped region.\n",
    "    \n",
    "    Parameters:\n",
    "      pdf_path (str): Path to the PDF file.\n",
    "      crop_top (int): Amount to crop from the top (in points).\n",
    "      start_page (int): Start extraction from this page (1-indexed).\n",
    "      end_page (int or None): End extraction at this page (1-indexed). If None, process until the end.\n",
    "      \n",
    "    Returns:\n",
    "      str: Concatenated text extracted from the cropped pages.\n",
    "    \"\"\"\n",
    "    reader = open_clean_pdf(pdf_path)\n",
    "    lines = []\n",
    "\n",
    "    if end_page is None:\n",
    "        end_page = len(reader.pages)\n",
    "    \n",
    "    for i in range(start_page - 1, end_page):\n",
    "        page = reader.pages[i]\n",
    "        text = page.extract_text()\n",
    "        if text:\n",
    "            for line in text.splitlines():\n",
    "                lines.append(line)\n",
    "    \n",
    "    return lines\n",
    "\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    \"\"\"\n",
    "    Splits text into sentences. The regex is adjusted to avoid splitting\n",
    "    on periods used within numbers (such as percentages or decimals).\n",
    "    \n",
    "    Parameters:\n",
    "      text (str): The text to split.\n",
    "      \n",
    "    Returns:\n",
    "      list: A list of individual sentences.\n",
    "    \"\"\"\n",
    "    normalized_text = \" \".join(text)\n",
    "    sentence_pattern = re.compile(r'(?<=[.!?])(?!\\s*\\d)')\n",
    "    sentences = sentence_pattern.split(normalized_text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "    english_sentences = []\n",
    "    for sentence in sentences:\n",
    "        try:\n",
    "            if detect(sentence) == \"en\":\n",
    "                english_sentences.append(sentence)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    return \"\\n\".join(english_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_df = pd.read_csv('Page.csv')\n",
    "page_df['filename'] = page_df['ticker'] + '_' + page_df['year'].astype(str)\n",
    "page_df = page_df[page_df['start'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in page_df['filename']:\n",
    "    start_page = int(page_df[page_df['filename'] == filename]['start'].values[0])\n",
    "    end_page = int(page_df[page_df['filename'] == filename]['end'].values[0])\n",
    "    try:\n",
    "        text = extract_text(f\"AnnualReport/{filename}.pdf\", start_page=start_page, end_page=end_page)\n",
    "    except:\n",
    "        print(f\"AnnualReport/{filename}.pdf\")\n",
    "        continue\n",
    "    sentences = split_into_sentences(text)\n",
    "    with open(f\"Text/{filename}.txt\", \"w\") as text_file:\n",
    "        text_file.write(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "for filename in page_df['filename']:\n",
    "    try:\n",
    "        line_count = 0\n",
    "        with open(f\"Text/{filename}.txt\", \"r\") as file:\n",
    "            for line in file:\n",
    "                line_count += 1\n",
    "        lines.append(line_count)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create PseudoLabel from L&M Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_dict = pd.read_csv('LM_Dict_2024.csv')\n",
    "lm_dict = lm_dict[(lm_dict['Positive'] > 0) | (lm_dict['Negative'] > 0) | (lm_dict['Uncertainty'] > 0) | (lm_dict['Litigious'] > 0) | (lm_dict['Strong_Modal'] > 0) | (lm_dict['Weak_Modal'] > 0) | (lm_dict['Constraining'] > 0)]\n",
    "lm_dict = lm_dict[['Word', 'Positive', 'Negative', 'Uncertainty', 'Litigious', 'Strong_Modal', 'Weak_Modal', 'Constraining']]\n",
    "lm_dict = lm_dict.melt(id_vars=['Word'], var_name='Label', value_name='Value')\n",
    "lm_dict = lm_dict[lm_dict['Value'] > 0]\n",
    "lm_dict = lm_dict[['Word', 'Label']]\n",
    "lm_dict = lm_dict.drop_duplicates()\n",
    "lm_dict = lm_dict.reset_index(drop=True)\n",
    "lm_dict.to_csv('LM_Sentiment_2024.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_dict = pd.read_csv('LM_Sentiment_2024.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 145/145 [17:05<00:00,  7.08s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Get unique labels from lm_dict\n",
    "unique_labels = lm_dict['Label'].unique().tolist()\n",
    "\n",
    "# Create an empty dataframe with columns: Sentence, Ticker, Year, plus all unique labels\n",
    "all_columns = ['Sentence', 'Ticker', 'Year'] + unique_labels\n",
    "text_df = pd.DataFrame(columns=all_columns)\n",
    "\n",
    "# Iterate over the Text files with a progress bar\n",
    "for filename in tqdm(page_df['filename'], desc=\"Processing files\"):\n",
    "    try:\n",
    "        with open(f\"Text/{filename}.txt\", \"r\") as file:\n",
    "            for line in file:\n",
    "                sentence = line.strip()\n",
    "                if sentence:\n",
    "                    # Initialize the row with default values (0 for each label)\n",
    "                    row_data = {\n",
    "                        \"Sentence\": sentence,\n",
    "                        \"Ticker\": filename.split('_')[0],\n",
    "                        \"Year\": filename.split('_')[1]\n",
    "                    }\n",
    "                    for label in unique_labels:\n",
    "                        row_data[label] = 0\n",
    "\n",
    "                    # For each label, if a word is found, mark the label and then continue to the next label.\n",
    "                    for label in unique_labels:\n",
    "                        words = lm_dict[lm_dict['Label'] == label]['Word']\n",
    "                        for word in words:\n",
    "                            if word in sentence.upper():\n",
    "                                row_data[label] = 1\n",
    "                                break  # Found a word for this label, continue to next label\n",
    "\n",
    "                    # Use pd.concat to add the row to the DataFrame\n",
    "                    new_row = pd.DataFrame([row_data])\n",
    "                    text_df = pd.concat([text_df, new_row], ignore_index=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file Text/{filename}.txt: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df.to_csv('Text_Database.csv', index=False, escapechar='\\\\')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = pd.read_csv('Text_Database.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_qwen = pd.read_csv('./Result/icl_multiclass_fin-r1.csv')\n",
    "mc_qwen['Strong_Modal_Pred'] = mc_qwen['Strong Modal_Pred']\n",
    "mc_qwen['Weak_Modal_Pred'] = mc_qwen['Weak Modal_Pred']\n",
    "mc_qwen = mc_qwen.drop(columns=['Unnamed: 0', 'index', 'Strong Modal_Pred', 'Weak Modal_Pred'])\n",
    "mc_qwen['Strong_Modal_Pred'] = mc_qwen['Strong_Modal_Pred'].fillna(0).astype(int)\n",
    "mc_qwen['Weak_Modal_Pred'] = mc_qwen['Weak_Modal_Pred'].fillna(0).astype(int)\n",
    "mc_qwen.to_csv('./Result/icl_multiclass_fin-r1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n",
      "0.28457339189741837\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.57      0.63     38908\n",
      "           1       0.59      0.72      0.65     33389\n",
      "\n",
      "    accuracy                           0.64     72297\n",
      "   macro avg       0.65      0.64      0.64     72297\n",
      "weighted avg       0.65      0.64      0.64     72297\n",
      "\n",
      "Negative\n",
      "0.08650148995328555\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.91      0.55     26522\n",
      "           1       0.80      0.20      0.32     45775\n",
      "\n",
      "    accuracy                           0.46     72297\n",
      "   macro avg       0.60      0.56      0.43     72297\n",
      "weighted avg       0.65      0.46      0.40     72297\n",
      "\n",
      "Uncertainty\n",
      "0.05479987376313433\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.32      0.47     60231\n",
      "           1       0.19      0.80      0.31     12066\n",
      "\n",
      "    accuracy                           0.40     72297\n",
      "   macro avg       0.54      0.56      0.39     72297\n",
      "weighted avg       0.77      0.40      0.44     72297\n",
      "\n",
      "Litigious\n",
      "0.05084420082900232\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.99      0.86     54842\n",
      "           1       0.66      0.04      0.08     17455\n",
      "\n",
      "    accuracy                           0.76     72297\n",
      "   macro avg       0.71      0.52      0.47     72297\n",
      "weighted avg       0.74      0.76      0.67     72297\n",
      "\n",
      "Strong_Modal\n",
      "0.10587858285319918\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.84      0.88     65012\n",
      "           1       0.17      0.29      0.22      7285\n",
      "\n",
      "    accuracy                           0.79     72297\n",
      "   macro avg       0.54      0.57      0.55     72297\n",
      "weighted avg       0.84      0.79      0.81     72297\n",
      "\n",
      "Weak_Modal\n",
      "0.10370592041562099\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97     69289\n",
      "           1       0.19      0.10      0.13      3008\n",
      "\n",
      "    accuracy                           0.94     72297\n",
      "   macro avg       0.57      0.54      0.55     72297\n",
      "weighted avg       0.93      0.94      0.94     72297\n",
      "\n",
      "Constraining\n",
      "0.13764569657803405\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.66      0.77     62181\n",
      "           1       0.22      0.57      0.31     10116\n",
      "\n",
      "    accuracy                           0.65     72297\n",
      "   macro avg       0.56      0.62      0.54     72297\n",
      "weighted avg       0.81      0.65      0.70     72297\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labels = ['Positive', 'Negative', 'Uncertainty', 'Litigious', 'Strong_Modal', 'Weak_Modal', 'Constraining']\n",
    "mc_fin = pd.read_csv('./Result/icl_multiclass_fin-r1.csv')\n",
    "for label in labels:\n",
    "    mc_fin[label + '_Pred'] = mc_fin[label + '_Pred'].replace(2, 1)\n",
    "\n",
    "for label in labels:\n",
    "    print(label)\n",
    "    print(cohen_kappa_score(mc_fin[label], mc_fin[label + '_Pred']))\n",
    "    print(classification_report(mc_fin[label], mc_fin[label + '_Pred'], zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n",
      "0.29563906648301763\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.65      0.67     38908\n",
      "           1       0.61      0.65      0.63     33389\n",
      "\n",
      "    accuracy                           0.65     72297\n",
      "   macro avg       0.65      0.65      0.65     72297\n",
      "weighted avg       0.65      0.65      0.65     72297\n",
      "\n",
      "Negative\n",
      "0.0589086576029324\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.98      0.55     26522\n",
      "           1       0.91      0.09      0.17     45775\n",
      "\n",
      "    accuracy                           0.42     72297\n",
      "   macro avg       0.65      0.54      0.36     72297\n",
      "weighted avg       0.72      0.42      0.31     72297\n",
      "\n",
      "Uncertainty\n",
      "0.14977244078001772\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.71      0.79     60231\n",
      "           1       0.26      0.49      0.34     12066\n",
      "\n",
      "    accuracy                           0.68     72297\n",
      "   macro avg       0.57      0.60      0.56     72297\n",
      "weighted avg       0.77      0.68      0.71     72297\n",
      "\n",
      "Litigious\n",
      "0.024079664114708632\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      1.00      0.86     54842\n",
      "           1       0.79      0.02      0.03     17455\n",
      "\n",
      "    accuracy                           0.76     72297\n",
      "   macro avg       0.78      0.51      0.45     72297\n",
      "weighted avg       0.77      0.76      0.66     72297\n",
      "\n",
      "Strong_Modal\n",
      "0.021664013158984807\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95     65012\n",
      "           1       0.40      0.01      0.03      7285\n",
      "\n",
      "    accuracy                           0.90     72297\n",
      "   macro avg       0.65      0.51      0.49     72297\n",
      "weighted avg       0.85      0.90      0.85     72297\n",
      "\n",
      "Weak_Modal\n",
      "0.03479431541057809\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98     69289\n",
      "           1       0.37      0.02      0.04      3008\n",
      "\n",
      "    accuracy                           0.96     72297\n",
      "   macro avg       0.66      0.51      0.51     72297\n",
      "weighted avg       0.93      0.96      0.94     72297\n",
      "\n",
      "Constraining\n",
      "0.21961612121223584\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.92      0.90     62181\n",
      "           1       0.36      0.28      0.32     10116\n",
      "\n",
      "    accuracy                           0.83     72297\n",
      "   macro avg       0.62      0.60      0.61     72297\n",
      "weighted avg       0.81      0.83      0.82     72297\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labels = ['Positive', 'Negative', 'Uncertainty', 'Litigious', 'Strong_Modal', 'Weak_Modal', 'Constraining']\n",
    "mc_fin = pd.read_csv('./Result/icl_multiclass_fin-r1.csv')\n",
    "for label in labels:\n",
    "    mc_fin[label + '_Pred'] = mc_fin[label + '_Pred'].replace(2, 0)\n",
    "\n",
    "for label in labels:\n",
    "    print(label)\n",
    "    print(cohen_kappa_score(mc_fin[label], mc_fin[label + '_Pred']))\n",
    "    print(classification_report(mc_fin[label], mc_fin[label + '_Pred'], zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_qwen = pd.read_csv('./Result/zs_multiclass_qwen.csv')\n",
    "mc_qwen['Strong_Modal_Pred'] = mc_qwen['Strong Modal_Pred']\n",
    "mc_qwen['Weak_Modal_Pred'] = mc_qwen['Weak Modal_Pred']\n",
    "mc_qwen = mc_qwen.drop(columns=['Unnamed: 0', 'index', 'Strong Modal_Pred', 'Weak Modal_Pred'])\n",
    "mc_qwen['Strong_Modal_Pred'] = mc_qwen['Strong_Modal_Pred'].fillna(0).astype(int)\n",
    "mc_qwen['Weak_Modal_Pred'] = mc_qwen['Weak_Modal_Pred'].fillna(0).astype(int)\n",
    "mc_qwen.to_csv('./Result/zs_multiclass_qwen.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "mc_fin = pd.read_csv('./Result/zs_multiclass_fin-r1.csv')\n",
    "mc_fin['Strong_Modal_Pred'] = mc_fin['Strong Modal_Pred']\n",
    "mc_fin['Weak_Modal_Pred'] = mc_fin['Weak Modal_Pred']\n",
    "mc_fin = mc_fin.drop(columns=['Unnamed: 0', 'index', 'Strong Modal_Pred', 'Weak Modal_Pred'])\n",
    "mc_fin['Strong_Modal_Pred'] = mc_fin['Strong_Modal_Pred'].fillna(0).astype(int)\n",
    "mc_fin['Weak_Modal_Pred'] = mc_fin['Weak_Modal_Pred'].fillna(0).astype(int)\n",
    "mc_fin.to_csv('./Result/zs_multiclass_fin-r1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n",
      "0.25444953566274786\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.44      0.55     38908\n",
      "           1       0.56      0.83      0.67     33389\n",
      "\n",
      "    accuracy                           0.62     72297\n",
      "   macro avg       0.65      0.63      0.61     72297\n",
      "weighted avg       0.66      0.62      0.60     72297\n",
      "\n",
      "Negative\n",
      "0.16819958160837312\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.80      0.56     26522\n",
      "           1       0.77      0.40      0.53     45775\n",
      "\n",
      "    accuracy                           0.55     72297\n",
      "   macro avg       0.60      0.60      0.55     72297\n",
      "weighted avg       0.65      0.55      0.54     72297\n",
      "\n",
      "Uncertainty\n",
      "0.02077795415171757\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.16      0.28     60231\n",
      "           1       0.18      0.89      0.29     12066\n",
      "\n",
      "    accuracy                           0.29     72297\n",
      "   macro avg       0.53      0.53      0.29     72297\n",
      "weighted avg       0.76      0.29      0.28     72297\n",
      "\n",
      "Litigious\n",
      "0.022329539033767887\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.99      0.86     54842\n",
      "           1       0.44      0.03      0.05     17455\n",
      "\n",
      "    accuracy                           0.76     72297\n",
      "   macro avg       0.60      0.51      0.45     72297\n",
      "weighted avg       0.68      0.76      0.66     72297\n",
      "\n",
      "Strong_Modal\n",
      "0.031873429349297155\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.99      0.94     65012\n",
      "           1       0.23      0.03      0.06      7285\n",
      "\n",
      "    accuracy                           0.89     72297\n",
      "   macro avg       0.57      0.51      0.50     72297\n",
      "weighted avg       0.83      0.89      0.85     72297\n",
      "\n",
      "Weak_Modal\n",
      "0.004417500137413377\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98     69289\n",
      "           1       0.17      0.00      0.01      3008\n",
      "\n",
      "    accuracy                           0.96     72297\n",
      "   macro avg       0.56      0.50      0.49     72297\n",
      "weighted avg       0.93      0.96      0.94     72297\n",
      "\n",
      "Constraining\n",
      "0.1958596562842343\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.91      0.90     62181\n",
      "           1       0.33      0.27      0.30     10116\n",
      "\n",
      "    accuracy                           0.82     72297\n",
      "   macro avg       0.61      0.59      0.60     72297\n",
      "weighted avg       0.81      0.82      0.81     72297\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labels = ['Positive', 'Negative', 'Uncertainty', 'Litigious', 'Strong_Modal', 'Weak_Modal', 'Constraining']\n",
    "mc_fin = pd.read_csv('./Result/zs_multiclass_fin-r1.csv')\n",
    "for label in labels:\n",
    "    mc_fin[label + '_Pred'] = mc_fin[label + '_Pred'].replace(2, 1)\n",
    "\n",
    "for label in labels:\n",
    "    print(label)\n",
    "    print(cohen_kappa_score(mc_fin[label], mc_fin[label + '_Pred']))\n",
    "    print(classification_report(mc_fin[label], mc_fin[label + '_Pred'], zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n",
      "0.2932349725419099\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.60      0.64     38908\n",
      "           1       0.60      0.70      0.65     33389\n",
      "\n",
      "    accuracy                           0.64     72297\n",
      "   macro avg       0.65      0.65      0.64     72297\n",
      "weighted avg       0.65      0.64      0.64     72297\n",
      "\n",
      "Negative\n",
      "0.12199040765257918\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.93      0.57     26522\n",
      "           1       0.85      0.23      0.36     45775\n",
      "\n",
      "    accuracy                           0.48     72297\n",
      "   macro avg       0.63      0.58      0.46     72297\n",
      "weighted avg       0.69      0.48      0.43     72297\n",
      "\n",
      "Uncertainty\n",
      "0.15596942304241612\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.79      0.82     60231\n",
      "           1       0.27      0.40      0.32     12066\n",
      "\n",
      "    accuracy                           0.72     72297\n",
      "   macro avg       0.57      0.59      0.57     72297\n",
      "weighted avg       0.77      0.72      0.74     72297\n",
      "\n",
      "Litigious\n",
      "0.007905041722925277\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      1.00      0.86     54842\n",
      "           1       0.70      0.01      0.01     17455\n",
      "\n",
      "    accuracy                           0.76     72297\n",
      "   macro avg       0.73      0.50      0.44     72297\n",
      "weighted avg       0.74      0.76      0.66     72297\n",
      "\n",
      "Strong_Modal\n",
      "0.0024660276591903507\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95     65012\n",
      "           1       1.00      0.00      0.00      7285\n",
      "\n",
      "    accuracy                           0.90     72297\n",
      "   macro avg       0.95      0.50      0.47     72297\n",
      "weighted avg       0.91      0.90      0.85     72297\n",
      "\n",
      "Weak_Modal\n",
      "-8.291518202740988e-05\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98     69289\n",
      "           1       0.00      0.00      0.00      3008\n",
      "\n",
      "    accuracy                           0.96     72297\n",
      "   macro avg       0.48      0.50      0.49     72297\n",
      "weighted avg       0.92      0.96      0.94     72297\n",
      "\n",
      "Constraining\n",
      "0.13426878549186705\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.97      0.92     62181\n",
      "           1       0.42      0.12      0.19     10116\n",
      "\n",
      "    accuracy                           0.85     72297\n",
      "   macro avg       0.64      0.55      0.55     72297\n",
      "weighted avg       0.81      0.85      0.82     72297\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labels = ['Positive', 'Negative', 'Uncertainty', 'Litigious', 'Strong_Modal', 'Weak_Modal', 'Constraining']\n",
    "mc_fin = pd.read_csv('./Result/zs_multiclass_fin-r1.csv')\n",
    "for label in labels:\n",
    "    mc_fin[label + '_Pred'] = mc_fin[label + '_Pred'].replace(2, 0)\n",
    "\n",
    "for label in labels:\n",
    "    print(label)\n",
    "    print(cohen_kappa_score(mc_fin[label], mc_fin[label + '_Pred']))\n",
    "    print(classification_report(mc_fin[label], mc_fin[label + '_Pred'], zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n",
      "0.25484814738791384\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.46      0.57     38908\n",
      "           1       0.56      0.80      0.66     33389\n",
      "\n",
      "    accuracy                           0.62     72297\n",
      "   macro avg       0.65      0.63      0.61     72297\n",
      "weighted avg       0.65      0.62      0.61     72297\n",
      "\n",
      "Negative\n",
      "0.09284094909645924\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.91      0.56     26522\n",
      "           1       0.80      0.21      0.33     45775\n",
      "\n",
      "    accuracy                           0.47     72297\n",
      "   macro avg       0.60      0.56      0.44     72297\n",
      "weighted avg       0.65      0.47      0.41     72297\n",
      "\n",
      "Uncertainty\n",
      "0.0784803049461178\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.39      0.54     60231\n",
      "           1       0.20      0.78      0.32     12066\n",
      "\n",
      "    accuracy                           0.45     72297\n",
      "   macro avg       0.55      0.58      0.43     72297\n",
      "weighted avg       0.78      0.45      0.51     72297\n",
      "\n",
      "Litigious\n",
      "0.025530579528387998\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.99      0.86     54842\n",
      "           1       0.46      0.03      0.05     17455\n",
      "\n",
      "    accuracy                           0.76     72297\n",
      "   macro avg       0.61      0.51      0.46     72297\n",
      "weighted avg       0.69      0.76      0.67     72297\n",
      "\n",
      "Strong_Modal\n",
      "0.09218881457718653\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.81      0.86     65012\n",
      "           1       0.16      0.32      0.21      7285\n",
      "\n",
      "    accuracy                           0.76     72297\n",
      "   macro avg       0.54      0.57      0.54     72297\n",
      "weighted avg       0.84      0.76      0.80     72297\n",
      "\n",
      "Weak_Modal\n",
      "-0.011837107172369965\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.88      0.92     69289\n",
      "           1       0.03      0.10      0.05      3008\n",
      "\n",
      "    accuracy                           0.85     72297\n",
      "   macro avg       0.50      0.49      0.48     72297\n",
      "weighted avg       0.92      0.85      0.88     72297\n",
      "\n",
      "Constraining\n",
      "0.2207864007633007\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.84      0.87     62181\n",
      "           1       0.30      0.43      0.35     10116\n",
      "\n",
      "    accuracy                           0.78     72297\n",
      "   macro avg       0.60      0.63      0.61     72297\n",
      "weighted avg       0.82      0.78      0.79     72297\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labels = ['Positive', 'Negative', 'Uncertainty', 'Litigious', 'Strong_Modal', 'Weak_Modal', 'Constraining']\n",
    "mc_qwen = pd.read_csv('./Result/zs_multiclass_qwen.csv')\n",
    "for label in labels:\n",
    "    mc_qwen[label + '_Pred'] = mc_qwen[label + '_Pred'].replace(2, 1)\n",
    "\n",
    "for label in labels:\n",
    "    print(label)\n",
    "    print(cohen_kappa_score(mc_qwen[label], mc_qwen[label + '_Pred']))\n",
    "    print(classification_report(mc_qwen[label], mc_qwen[label + '_Pred'], zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n",
      "0.30086851038582274\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.64      0.66     38908\n",
      "           1       0.61      0.66      0.64     33389\n",
      "\n",
      "    accuracy                           0.65     72297\n",
      "   macro avg       0.65      0.65      0.65     72297\n",
      "weighted avg       0.65      0.65      0.65     72297\n",
      "\n",
      "Negative\n",
      "0.047509694634182975\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.99      0.55     26522\n",
      "           1       0.91      0.08      0.14     45775\n",
      "\n",
      "    accuracy                           0.41     72297\n",
      "   macro avg       0.64      0.53      0.35     72297\n",
      "weighted avg       0.71      0.41      0.29     72297\n",
      "\n",
      "Uncertainty\n",
      "0.13146633719569734\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.70      0.78     60231\n",
      "           1       0.24      0.48      0.32     12066\n",
      "\n",
      "    accuracy                           0.67     72297\n",
      "   macro avg       0.56      0.59      0.55     72297\n",
      "weighted avg       0.77      0.67      0.70     72297\n",
      "\n",
      "Litigious\n",
      "0.018452841101250672\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      1.00      0.86     54842\n",
      "           1       0.83      0.01      0.03     17455\n",
      "\n",
      "    accuracy                           0.76     72297\n",
      "   macro avg       0.80      0.51      0.44     72297\n",
      "weighted avg       0.78      0.76      0.66     72297\n",
      "\n",
      "Strong_Modal\n",
      "0.018074333963725153\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95     65012\n",
      "           1       0.44      0.01      0.02      7285\n",
      "\n",
      "    accuracy                           0.90     72297\n",
      "   macro avg       0.67      0.51      0.48     72297\n",
      "weighted avg       0.85      0.90      0.85     72297\n",
      "\n",
      "Weak_Modal\n",
      "0.013863971457770852\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98     69289\n",
      "           1       0.12      0.01      0.02      3008\n",
      "\n",
      "    accuracy                           0.96     72297\n",
      "   macro avg       0.54      0.50      0.50     72297\n",
      "weighted avg       0.92      0.96      0.94     72297\n",
      "\n",
      "Constraining\n",
      "0.22311285680481252\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.92      0.90     62181\n",
      "           1       0.36      0.28      0.32     10116\n",
      "\n",
      "    accuracy                           0.83     72297\n",
      "   macro avg       0.62      0.60      0.61     72297\n",
      "weighted avg       0.81      0.83      0.82     72297\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labels = ['Positive', 'Negative', 'Uncertainty', 'Litigious', 'Strong_Modal', 'Weak_Modal', 'Constraining']\n",
    "mc_qwen = pd.read_csv('./Result/zs_multiclass_qwen.csv')\n",
    "for label in labels:\n",
    "    mc_qwen[label + '_Pred'] = mc_qwen[label + '_Pred'].replace(2, 0)\n",
    "\n",
    "for label in labels:\n",
    "    print(label)\n",
    "    print(cohen_kappa_score(mc_qwen[label], mc_qwen[label + '_Pred']))\n",
    "    print(classification_report(mc_qwen[label], mc_qwen[label + '_Pred'], zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_qwen = pd.read_csv('./Result/zs_multilabel_qwen.csv')\n",
    "ml_qwen['Strong_Modal_Pred'] = ml_qwen['Strong Modal_Pred']\n",
    "ml_qwen['Weak_Modal_Pred'] = ml_qwen['Weak Modal_Pred']\n",
    "ml_qwen = ml_qwen.drop(columns=['Unnamed: 0', 'index', 'Strong Modal_Pred', 'Weak Modal_Pred'])\n",
    "ml_qwen['Strong_Modal_Pred'] = ml_qwen['Strong_Modal_Pred'].fillna(0).astype(int)\n",
    "ml_qwen['Weak_Modal_Pred'] = ml_qwen['Weak_Modal_Pred'].fillna(0).astype(int)\n",
    "ml_qwen.to_csv('./Result/zs_multilabel_qwen.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n",
      "0.3600412035564151\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.84      0.74     38908\n",
      "           1       0.73      0.52      0.60     33389\n",
      "\n",
      "    accuracy                           0.69     72297\n",
      "   macro avg       0.70      0.68      0.67     72297\n",
      "weighted avg       0.70      0.69      0.68     72297\n",
      "\n",
      "Negative\n",
      "0.10200101335204914\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.95      0.57     26522\n",
      "           1       0.87      0.18      0.30     45775\n",
      "\n",
      "    accuracy                           0.46     72297\n",
      "   macro avg       0.63      0.57      0.43     72297\n",
      "weighted avg       0.70      0.46      0.39     72297\n",
      "\n",
      "Uncertainty\n",
      "0.1664291993585736\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.83      0.85     60231\n",
      "           1       0.29      0.35      0.32     12066\n",
      "\n",
      "    accuracy                           0.75     72297\n",
      "   macro avg       0.58      0.59      0.58     72297\n",
      "weighted avg       0.77      0.75      0.76     72297\n",
      "\n",
      "Litigious\n",
      "0.008307171311496275\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      1.00      0.86     54842\n",
      "           1       0.86      0.01      0.01     17455\n",
      "\n",
      "    accuracy                           0.76     72297\n",
      "   macro avg       0.81      0.50      0.44     72297\n",
      "weighted avg       0.78      0.76      0.66     72297\n",
      "\n",
      "Strong_Modal\n",
      "0.024782265556539618\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95     65012\n",
      "           1       0.37      0.02      0.03      7285\n",
      "\n",
      "    accuracy                           0.90     72297\n",
      "   macro avg       0.63      0.51      0.49     72297\n",
      "weighted avg       0.85      0.90      0.85     72297\n",
      "\n",
      "Weak_Modal\n",
      "0.04011119142723718\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.64      0.77     69289\n",
      "           1       0.06      0.54      0.11      3008\n",
      "\n",
      "    accuracy                           0.64     72297\n",
      "   macro avg       0.52      0.59      0.44     72297\n",
      "weighted avg       0.93      0.64      0.75     72297\n",
      "\n",
      "Constraining\n",
      "0.20895409027881862\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.93      0.91     62181\n",
      "           1       0.37      0.25      0.30     10116\n",
      "\n",
      "    accuracy                           0.83     72297\n",
      "   macro avg       0.62      0.59      0.60     72297\n",
      "weighted avg       0.81      0.83      0.82     72297\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labels = ['Positive', 'Negative', 'Uncertainty', 'Litigious', 'Strong_Modal', 'Weak_Modal', 'Constraining']\n",
    "ml_qwen = pd.read_csv('./Result/zs_multilabel_qwen.csv')\n",
    "\n",
    "for label in labels:\n",
    "    print(label)\n",
    "    print(cohen_kappa_score(ml_qwen[label], ml_qwen[label + '_Pred']))\n",
    "    print(classification_report(ml_qwen[label], ml_qwen[label + '_Pred'], zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_fin = pd.read_csv('./Result/icl_multilabel_fin-r1.csv')\n",
    "ml_fin['Strong_Modal_Pred'] = ml_fin['Strong Modal_Pred']\n",
    "ml_fin['Weak_Modal_Pred'] = ml_fin['Weak Modal_Pred']\n",
    "ml_fin = ml_fin.drop(columns=['Unnamed: 0', 'index', 'Strong Modal_Pred', 'Weak Modal_Pred'])\n",
    "ml_fin['Strong_Modal_Pred'] = ml_fin['Strong_Modal_Pred'].fillna(0).astype(int)\n",
    "ml_fin['Weak_Modal_Pred'] = ml_fin['Weak_Modal_Pred'].fillna(0).astype(int)\n",
    "ml_fin.to_csv('./Result/icl_multilabel_fin-r1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n",
      "0.3429179132951464\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.64      0.68     38908\n",
      "           1       0.63      0.70      0.66     33389\n",
      "\n",
      "    accuracy                           0.67     72297\n",
      "   macro avg       0.67      0.67      0.67     72297\n",
      "weighted avg       0.68      0.67      0.67     72297\n",
      "\n",
      "Negative\n",
      "0.09205831609887727\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.97      0.56     26522\n",
      "           1       0.90      0.15      0.25     45775\n",
      "\n",
      "    accuracy                           0.45     72297\n",
      "   macro avg       0.65      0.56      0.41     72297\n",
      "weighted avg       0.72      0.45      0.37     72297\n",
      "\n",
      "Uncertainty\n",
      "0.20178214690498497\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.84      0.86     60231\n",
      "           1       0.32      0.37      0.34     12066\n",
      "\n",
      "    accuracy                           0.77     72297\n",
      "   macro avg       0.60      0.61      0.60     72297\n",
      "weighted avg       0.78      0.77      0.77     72297\n",
      "\n",
      "Litigious\n",
      "0.029406946319889382\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      1.00      0.86     54842\n",
      "           1       0.87      0.02      0.04     17455\n",
      "\n",
      "    accuracy                           0.76     72297\n",
      "   macro avg       0.81      0.51      0.45     72297\n",
      "weighted avg       0.79      0.76      0.67     72297\n",
      "\n",
      "Strong_Modal\n",
      "0.20467240856014646\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.93      0.92     65012\n",
      "           1       0.29      0.27      0.28      7285\n",
      "\n",
      "    accuracy                           0.86     72297\n",
      "   macro avg       0.61      0.60      0.60     72297\n",
      "weighted avg       0.86      0.86      0.86     72297\n",
      "\n",
      "Weak_Modal\n",
      "0.08650251487955962\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.82      0.89     69289\n",
      "           1       0.09      0.41      0.15      3008\n",
      "\n",
      "    accuracy                           0.81     72297\n",
      "   macro avg       0.53      0.62      0.52     72297\n",
      "weighted avg       0.93      0.81      0.86     72297\n",
      "\n",
      "Constraining\n",
      "0.26555673366678123\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.90      0.90     62181\n",
      "           1       0.37      0.37      0.37     10116\n",
      "\n",
      "    accuracy                           0.82     72297\n",
      "   macro avg       0.63      0.63      0.63     72297\n",
      "weighted avg       0.82      0.82      0.82     72297\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labels = ['Positive', 'Negative', 'Uncertainty', 'Litigious', 'Strong_Modal', 'Weak_Modal', 'Constraining']\n",
    "ml_fin = pd.read_csv('./Result/icl_multilabel_fin-r1.csv')\n",
    "\n",
    "for label in labels:\n",
    "    print(label)\n",
    "    print(cohen_kappa_score(ml_fin[label], ml_fin[label + '_Pred']))\n",
    "    print(classification_report(ml_fin[label], ml_fin[label + '_Pred'], zero_division=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
